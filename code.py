# -*- coding: utf-8 -*-
"""DMT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tEP0g1n95zvKzgPLY2reyTi6GzT0-iDS

**Hybrid XGBoost and Neural Network**
"""

# Install required libraries
#!pip install xgboost tensorflow

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)

# Train a simple Neural Network using TensorFlow
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1)
])
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
nn_predictions = model.predict(X_test).flatten()

# Combine predictions from both models
hybrid_predictions = (xgb_predictions + nn_predictions) / 2

# Calculate RMSE for individual models
xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)
nn_rmse = mean_squared_error(y_test, nn_predictions, squared=False)

# Calculate MAE for individual models
xgb_mae = mean_absolute_error(y_test, xgb_predictions)
nn_mae = mean_absolute_error(y_test, nn_predictions)

# Calculate R2 score for individual models
xgb_r2 = r2_score(y_test, xgb_predictions)
nn_r2 = r2_score(y_test, nn_predictions)

# Calculate RMSE for hybrid approach
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for hybrid approach
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for hybrid approach
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate accuracy percentage based on RMSE
def calculate_accuracy_percentage(true_values, predicted_values):
    return ((1 - (np.sqrt(np.mean((true_values - predicted_values)**2)) / np.mean(true_values))) * 100).round(2)

# Calculate accuracy percentage for individual models
xgb_accuracy_percentage = calculate_accuracy_percentage(y_test, xgb_predictions)
nn_accuracy_percentage = calculate_accuracy_percentage(y_test, nn_predictions)
hybrid_accuracy_percentage = calculate_accuracy_percentage(y_test, hybrid_predictions)

# Print the evaluation metrics
print("XGBoost Model - RMSE:", xgb_rmse, " - MAE:", xgb_mae, " - R2 Score:", xgb_r2, " - Accuracy Percentage:", xgb_accuracy_percentage, "%")
print("Neural Network Model - RMSE:", nn_rmse, " - MAE:", nn_mae, " - R2 Score:", nn_r2, " - Accuracy Percentage:", nn_accuracy_percentage, "%")
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")


# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid Gradient Boosting and k-Nearest Neighbors**"""

# Install required libraries
#pip install xgboost scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Gradient Boosting model
gbm_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gbm_model.fit(X_train, y_train)
gbm_predictions = gbm_model.predict(X_test)

# Train a k-Nearest Neighbors (k-NN) model
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_predictions = knn_model.predict(X_test)

# Combine predictions from both models using simple averaging
hybrid_predictions = (gbm_predictions + knn_predictions) / 2

# Calculate RMSE for individual models
gbm_rmse = mean_squared_error(y_test, gbm_predictions, squared=False)
knn_rmse = mean_squared_error(y_test, knn_predictions, squared=False)

# Calculate MAE for individual models
gbm_mae = mean_absolute_error(y_test, gbm_predictions)
knn_mae = mean_absolute_error(y_test, knn_predictions)

# Calculate R2 score for individual models
gbm_r2 = r2_score(y_test, gbm_predictions)
knn_r2 = r2_score(y_test, knn_predictions)

# Calculate RMSE for hybrid approach
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for hybrid approach
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for hybrid approach
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate accuracy percentage based on RMSE
def calculate_accuracy_percentage(true_values, predicted_values):
    return ((1 - (np.sqrt(np.mean((true_values - predicted_values)**2)) / np.mean(true_values))) * 100).round(2)

# Calculate accuracy percentage for individual models
gbm_accuracy_percentage = calculate_accuracy_percentage(y_test, gbm_predictions)
knn_accuracy_percentage = calculate_accuracy_percentage(y_test, knn_predictions)
hybrid_accuracy_percentage = calculate_accuracy_percentage(y_test, hybrid_predictions)

# Print the evaluation metrics
print("Gradient Boosting Model - RMSE:", gbm_rmse, " - MAE:", gbm_mae, " - R2 Score:", gbm_r2, " - Accuracy Percentage:", gbm_accuracy_percentage, "%")
print("k-Nearest Neighbors Model - RMSE:", knn_rmse, " - MAE:", knn_mae, " - R2 Score:", knn_r2, " - Accuracy Percentage:", knn_accuracy_percentage, "%")
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid Random Forest and SVM**"""

# Install required libraries

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Train a Support Vector Machine (SVM) model
svm_model = SVR(kernel='linear')
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# Combine predictions from both models using simple averaging
hybrid_predictions = (rf_predictions + svm_predictions) / 2

# Calculate RMSE for individual models
rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)
svm_rmse = mean_squared_error(y_test, svm_predictions, squared=False)

# Calculate MAE for individual models
rf_mae = mean_absolute_error(y_test, rf_predictions)
svm_mae = mean_absolute_error(y_test, svm_predictions)

# Calculate R2 score for individual models
rf_r2 = r2_score(y_test, rf_predictions)
svm_r2 = r2_score(y_test, svm_predictions)

# Calculate RMSE for hybrid approach
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for hybrid approach
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for hybrid approach
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate accuracy percentage based on RMSE
def calculate_accuracy_percentage(true_values, predicted_values):
    return ((1 - (np.sqrt(np.mean((true_values - predicted_values)**2)) / np.mean(true_values))) * 100).round(2)

# Calculate accuracy percentage for individual models
rf_accuracy_percentage = calculate_accuracy_percentage(y_test, rf_predictions)
svm_accuracy_percentage = calculate_accuracy_percentage(y_test, svm_predictions)
hybrid_accuracy_percentage = calculate_accuracy_percentage(y_test, hybrid_predictions)

# Print the evaluation metrics
print("Random Forest Model - RMSE:", rf_rmse, " - MAE:", rf_mae, " - R2 Score:", rf_r2, " - Accuracy Percentage:", rf_accuracy_percentage, "%")
print("SVM Model - RMSE:", svm_rmse, " - MAE:", svm_mae, " - R2 Score:", svm_r2, " - Accuracy Percentage:", svm_accuracy_percentage, "%")
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid Support Vector Machine (SVM), Neural Network and Gradient Boosting model**"""

# Install required libraries
#!pip install scikit-learn tensorflow xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)

X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Support Vector Machine (SVM) model
svm_model = SVR(kernel='linear')
svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

# Train a Neural Network using TensorFlow
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1)
])
nn_model.compile(optimizer='adam', loss='mean_squared_error')
nn_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
nn_predictions = nn_model.predict(X_test).flatten()

# Train an XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)

# Combine predictions from all three models using averaging
hybrid_predictions = (svm_predictions + nn_predictions + xgb_predictions) / 3

# Calculate RMSE for individual models
svm_rmse = mean_squared_error(y_test, svm_predictions, squared=False)
nn_rmse = mean_squared_error(y_test, nn_predictions, squared=False)
xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)

# Calculate MAE for individual models
svm_mae = mean_absolute_error(y_test, svm_predictions)
nn_mae = mean_absolute_error(y_test, nn_predictions)
xgb_mae = mean_absolute_error(y_test, xgb_predictions)

# Calculate R2 score for individual models
svm_r2 = r2_score(y_test, svm_predictions)
nn_r2 = r2_score(y_test, nn_predictions)
xgb_r2 = r2_score(y_test, xgb_predictions)

# Calculate RMSE for hybrid approach
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for hybrid approach
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for hybrid approach
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate accuracy percentage based on RMSE
def calculate_accuracy_percentage(true_values, predicted_values):
    return ((1 - (np.sqrt(np.mean((true_values - predicted_values)**2)) / np.mean(true_values))) * 100).round(2)

# Calculate accuracy percentage for individual models
svm_accuracy_percentage = calculate_accuracy_percentage(y_test, svm_predictions)
nn_accuracy_percentage = calculate_accuracy_percentage(y_test, nn_predictions)
xgb_accuracy_percentage = calculate_accuracy_percentage(y_test, xgb_predictions)
hybrid_accuracy_percentage = calculate_accuracy_percentage(y_test, hybrid_predictions)

# Print the evaluation metrics
print("SVM Model - RMSE:", svm_rmse, " - MAE:", svm_mae, " - R2 Score:", svm_r2, " - Accuracy Percentage:", svm_accuracy_percentage, "%")
print("Neural Network Model - RMSE:", nn_rmse, " - MAE:", nn_mae, " - R2 Score:", nn_r2, " - Accuracy Percentage:", nn_accuracy_percentage, "%")
print("XGBoost Model - RMSE:", xgb_rmse, " - MAE:", xgb_mae, " - R2 Score:", xgb_r2, " - Accuracy Percentage:", xgb_accuracy_percentage, "%")
print("Complex Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid Ridge Regression model, Long Short-Term Memory (LSTM) model and LightGBM model**"""

# Install required libraries
#!pip install scikit-learn tensorflow lightgbm

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)

X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Ridge Regression model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
ridge_predictions = ridge_model.predict(X_test)

# Train a Long Short-Term Memory (LSTM) model using TensorFlow
lstm_model = Sequential([
    LSTM(64, activation='relu', input_shape=(X_train.shape[1], 1)),
    Dense(1)
])
lstm_model.compile(optimizer='adam', loss='mean_squared_error')
X_train_lstm = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_lstm = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)
lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)
lstm_predictions = lstm_model.predict(X_test_lstm).flatten()

# Train a LightGBM model
lgb_train = lgb.Dataset(X_train, label=y_train)
lgb_params = {'objective': 'regression', 'num_leaves': 31, 'learning_rate': 0.05}
lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)
lgb_predictions = lgb_model.predict(X_test)

# Combine predictions from all three models using weighted averaging
hybrid_predictions = (0.4 * ridge_predictions + 0.3 * lstm_predictions + 0.3 * lgb_predictions)

# Calculate RMSE for individual models
ridge_rmse = mean_squared_error(y_test, ridge_predictions, squared=False)
lstm_rmse = mean_squared_error(y_test, lstm_predictions, squared=False)
lgb_rmse = mean_squared_error(y_test, lgb_predictions, squared=False)

# Calculate RMSE for hybrid approach
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for individual models
ridge_mae = mean_absolute_error(y_test, ridge_predictions)
lstm_mae = mean_absolute_error(y_test, lstm_predictions)
lgb_mae = mean_absolute_error(y_test, lgb_predictions)

# Calculate R2 score for individual models
ridge_r2 = r2_score(y_test, ridge_predictions)
lstm_r2 = r2_score(y_test, lstm_predictions)
lgb_r2 = r2_score(y_test, lgb_predictions)

# Calculate RMSE, MAE, R2 score, and accuracy percentage for hybrid approach
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)
hybrid_r2 = r2_score(y_test, hybrid_predictions)
accuracy_percentage = ((1 - (hybrid_rmse / y_test.mean())) * 100).round(2)

# Print the evaluation metrics
print("\nRMSE for Ridge Regression Model:", ridge_rmse)
print("RMSE for LSTM Model:", lstm_rmse)
print("RMSE for LightGBM Model:", lgb_rmse)
print("RMSE for Complex Hybrid Model:", hybrid_rmse)

print("\nMAE for Ridge Regression Model:", ridge_mae)
print("MAE for LSTM Model:", lstm_mae)
print("MAE for LightGBM Model:", lgb_mae)
print("MAE for Complex Hybrid Model:", hybrid_mae)

print("\nR2 Score for Ridge Regression Model:", ridge_r2)
print("R2 Score for LSTM Model:", lstm_r2)
print("R2 Score for LightGBM Model:", lgb_r2)
print("R2 Score for Complex Hybrid Model:", hybrid_r2)

print("\nAccuracy Percentage for Complex Hybrid Model:", accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Particle Swarm Optimization (PSO) and Tabu Search algorithm**"""

# Install required libraries

!pip install scikit-learn pyswarm tabulate

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from pyswarm import pso
from tabulate import tabulate
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)

X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base models
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
base_models = [rf_model]  # Add other base models here if needed

# Train the base models
for model in base_models:
    model.fit(X_train, y_train)

# Define the fitness function for PSO
def fitness_function(weights):
    ensemble_predictions = sum(w * model.predict(X_test) for w, model in zip(weights, base_models))
    mse = mean_squared_error(y_test, ensemble_predictions)
    return mse

# Number of base models
num_models = len(base_models)

# Initialize PSO optimization parameters
lb = [0.0] * num_models  # Lower bounds for weights
ub = [1.0] * num_models  # Upper bounds for weights

# Perform PSO optimization to find optimal weights
best_weights, _ = pso(fitness_function, lb, ub, swarmsize=20, maxiter=50)

# Combine predictions using optimized weights
ensemble_predictions = sum(w * model.predict(X_test) for w, model in zip(best_weights, base_models))

# Calculate RMSE for the ensemble
ensemble_rmse = mean_squared_error(y_test, ensemble_predictions, squared=False)

# Calculate MAE for the ensemble
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)

# Calculate R2 score for the ensemble
ensemble_r2 = r2_score(y_test, ensemble_predictions)

# Calculate accuracy percentage based on RMSE
accuracy_percentage = ((1 - (ensemble_rmse / y_test.mean())) * 100).round(2)

# Display the optimized weights, RMSE, MAE, R2 score, and accuracy percentage
results = [{'Model': f'Model {i+1}', 'Weight': weight} for i, weight in enumerate(best_weights)]
results.append({'Model': 'Ensemble', 'Weight': ''})
results.append({'Model': 'Ensemble RMSE', 'Weight': ensemble_rmse})
results.append({'Model': 'Ensemble MAE', 'Weight': ensemble_mae})
results.append({'Model': 'Ensemble R2 Score', 'Weight': ensemble_r2})
results.append({'Model': 'Accuracy Percentage', 'Weight': accuracy_percentage})

print(tabulate(results, headers='keys', tablefmt='grid'))
print("Accuracy Percentage for Complex Hybrid Model:", accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid RandomForest and XGBoost**"""

# Install required libraries
!pip install xgboost scikit-learn

# Import the necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset and preprocess
data = pd.read_csv('/content/encoded2_dataset.csv')

# Split the data into features (X) and target variable (2025 Rank)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Define and train the XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)

# ... (Rest of the code for evaluation and hyperparameter tuning)


# Hyperparameter tuning for Random Forest
from sklearn.model_selection import GridSearchCV

# Define hyperparameters to tune for Random Forest
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Create GridSearchCV object for Random Forest
rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=5)

# Perform the grid search
rf_grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_rf_params = rf_grid_search.best_params_

# Train the Random Forest model with the best hyperparameters
best_rf_model = RandomForestRegressor(**best_rf_params)
best_rf_model.fit(X_train, y_train)

# Hyperparameter tuning for XGBoost
xgb_param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

# Create GridSearchCV object for XGBoost
xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=5)

# Perform the grid search
xgb_grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_xgb_params = xgb_grid_search.best_params_

# Train the XGBoost model with the best hyperparameters
best_xgb_model = xgb.XGBRegressor(**best_xgb_params)
best_xgb_model.fit(X_train, y_train)

# ... (Evaluation of the models with best hyperparameters)



# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Train an XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)

# Calculate RMSE, MAE, and R2 score for Random Forest
rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

# Calculate RMSE, MAE, and R2 score for XGBoost
xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)
xgb_mae = mean_absolute_error(y_test, xgb_predictions)
xgb_r2 = r2_score(y_test, xgb_predictions)

# Combine predictions from both models using simple averaging
hybrid_predictions = (rf_predictions + xgb_predictions) / 2


# Calculate RMSE, MAE, and R2 score for the hybrid model
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate the accuracy percentage for Random Forest
threshold = 0.5  # Set a threshold for ranking prediction
rf_accuracy = accuracy_score(y_test <= threshold, rf_predictions <= threshold) * 100

# Calculate the accuracy percentage for XGBoost
xgb_accuracy = accuracy_score(y_test <= threshold, xgb_predictions <= threshold) * 100

# Calculate the accuracy percentage for the hybrid model
hybrid_accuracy = accuracy_score(y_test <= threshold, hybrid_predictions <= threshold) * 100

# Print evaluation metrics for Random Forest model
print("\nRandom Forest Model:")
print("RMSE:", rf_rmse)
print("MAE:", rf_mae)
print("R2 Score:", rf_r2)
print("Accuracy:", rf_accuracy, "%")

# Print evaluation metrics for XGBoost model
print("\nXGBoost Model:")
print("RMSE:", xgb_rmse)
print("MAE:", xgb_mae)
print("R2 Score:", xgb_r2)
print("Accuracy:", xgb_accuracy, "%")

# Print evaluation metrics for the hybrid model
print("\nHybrid Model (Random Forest + XGBoost):")
print("RMSE:", hybrid_rmse)
print("MAE:", hybrid_mae)
print("R2 Score:", hybrid_r2)
print("Accuracy:", hybrid_accuracy, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid LinearRegression and RandomForest**"""

# Install required libraries
#!pip install scikit-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset and preprocess
data = pd.read_csv('/content/encoded2_dataset.csv')

# Split the data into features (X) and target variable (2025 Rank)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_predictions = rf_model.predict(X_test)

# Combine predictions from both models using simple averaging
hybrid_predictions = (lr_predictions + rf_predictions) / 2

# Calculate RMSE, MAE, and R2 score for Linear Regression model
lr_rmse = mean_squared_error(y_test, lr_predictions, squared=False)
lr_mae = mean_absolute_error(y_test, lr_predictions)
lr_r2 = r2_score(y_test, lr_predictions)

# Calculate RMSE, MAE, and R2 score for Random Forest model
rf_rmse = mean_squared_error(y_test, rf_predictions, squared=False)
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

# Calculate RMSE, MAE, and R2 score for the hybrid model
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate the accuracy percentage for Linear Regression
threshold = 0.5  # Set a threshold for ranking prediction
lr_accuracy = accuracy_score(y_test <= threshold, lr_predictions <= threshold) * 100

# Calculate the accuracy percentage for Random Forest
rf_accuracy = accuracy_score(y_test <= threshold, rf_predictions <= threshold) * 100

# Calculate the accuracy percentage for the hybrid model
hybrid_accuracy = accuracy_score(y_test <= threshold, hybrid_predictions <= threshold) * 100

# Print evaluation metrics for Linear Regression model
print("\nLinear Regression Model:")
print("RMSE:", lr_rmse)
print("MAE:", lr_mae)
print("R2 Score:", lr_r2)
print("Accuracy:", lr_accuracy, "%")

# Print evaluation metrics for Random Forest model
print("\nRandom Forest Model:")
print("RMSE:", rf_rmse)
print("MAE:", rf_mae)
print("R2 Score:", rf_r2)
print("Accuracy:", rf_accuracy, "%")

# Print evaluation metrics for the hybrid model
print("\nHybrid Model (Linear Regression + Random Forest):")
print("RMSE:", hybrid_rmse)
print("MAE:", hybrid_mae)
print("R2 Score:", hybrid_r2)
print("Accuracy:", hybrid_accuracy, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**Hybrid LinearRegression and XGBoost**"""

# Install required libraries
#!pip install scikit-learn xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset and preprocess
data = pd.read_csv('/content/encoded2_dataset.csv')

# Split the data into features (X) and target variable (2025 Rank)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Train an XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)


# Combine predictions from both models using simple averaging
hybrid_predictions = (lr_predictions + xgb_predictions) / 2

# Calculate RMSE for Linear Regression model
lr_rmse = mean_squared_error(y_test, lr_predictions, squared=False)

# Calculate RMSE for XGBoost model
xgb_rmse = mean_squared_error(y_test, xgb_predictions, squared=False)

# Calculate RMSE for hybrid model
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for Linear Regression model
lr_mae = mean_absolute_error(y_test, lr_predictions)

# Calculate MAE for XGBoost model
xgb_mae = mean_absolute_error(y_test, xgb_predictions)

# Calculate MAE for hybrid model
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for Linear Regression model
lr_r2 = r2_score(y_test, lr_predictions)

# Calculate R2 score for XGBoost model
xgb_r2 = r2_score(y_test, xgb_predictions)

# Calculate R2 score for hybrid model
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Calculate accuracy percentage for Linear Regression model
threshold = 0.5  # Set a threshold for ranking prediction
lr_accuracy = accuracy_score(y_test <= threshold, lr_predictions <= threshold) * 100

# Calculate accuracy percentage for XGBoost model
xgb_accuracy = accuracy_score(y_test <= threshold, xgb_predictions <= threshold) * 100

# Calculate accuracy percentage for hybrid model
hybrid_accuracy = accuracy_score(y_test <= threshold, hybrid_predictions <= threshold) * 100

# Print evaluation metrics for Linear Regression model
print("\nLinear Regression Model:")
print("RMSE:", lr_rmse)
print("MAE:", lr_mae)
print("R2 Score:", lr_r2)
print("Accuracy:", lr_accuracy, "%")

# Print evaluation metrics for XGBoost model
print("\nXGBoost Model:")
print("RMSE:", xgb_rmse)
print("MAE:", xgb_mae)
print("R2 Score:", xgb_r2)
print("Accuracy:", xgb_accuracy, "%")

# Print evaluation metrics for the hybrid model
print("\nHybrid Model (Linear Regression + XGBoost):")
print("RMSE:", hybrid_rmse)
print("MAE:", hybrid_mae)
print("R2 Score:", hybrid_r2)
print("Accuracy:", hybrid_accuracy, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

# Install required libraries
#!pip install scikit-learn xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
from sklearn.ensemble import VotingRegressor

# Load the dataset and preprocess
data = pd.read_csv('/content/encoded2_dataset.csv')
#data = pd.read_csv('/content/heart_failure_clinical_records_dataset.csv')
#data = pd.read_csv('/content/countries-aggregated.csv')

# Split the data into features (X) and the target variable (Overall SCORE)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Convert 'Date' to datetime and extract numeric features
#data['Date'] = pd.to_datetime(data['Date'])
#data['Year'] = data['Date'].dt.year
#data['Month'] = data['Date'].dt.month
#data['Day'] = data['Date'].dt.day

#Drop 'Date' column and use 'Year', 'Month', and 'Day' as features
#data = data.drop(['Date'], axis=1)

# Split the data into features (X) and the target variable (Deaths)
#X = data[['age', 'anaemia','creatinine_phosphokinase',	'diabetes',	'ejection_fraction',	'high_blood_pressure',	'platelets',	'serum_creatinine', 'serum_sodium',	'sex',	'smoking', 'time']]
#y = data['DEATH_EVENT']

# Split the data into features (X) and the target variable (Deaths)
#X = data[['Year', 'Month', 'Day', 'Confirmed', 'Recovered']]
#y = data['Deaths']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Train an XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Store the RMSE values during training
train_rmse_values = []

for epoch in range(1, 101):
    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, early_stopping_rounds=10)
    y_pred = xgb_model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    train_rmse_values.append(rmse)

# Combine predictions from both models using simple averaging
lr_predictions = lr_model.predict(X_test)  # Predictions from Linear Regression model
xgb_predictions = xgb_model.predict(X_test)  # Predictions from XGBoost model
hybrid_predictions = (lr_predictions + xgb_predictions) / 2

# Calculate RMSE for Linear Regression model
lr_rmse = mean_squared_error(y_test, lr_predictions, squared=False)

# Calculate RMSE for hybrid model
hybrid_rmse = mean_squared_error(y_test, hybrid_predictions, squared=False)

# Calculate MAE for Linear Regression model
lr_mae = mean_absolute_error(y_test, lr_predictions)

# Calculate MAE for hybrid model
hybrid_mae = mean_absolute_error(y_test, hybrid_predictions)

# Calculate R2 score for Linear Regression model
lr_r2 = r2_score(y_test, lr_predictions)

# Calculate R2 score for hybrid model
hybrid_r2 = r2_score(y_test, hybrid_predictions)

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2)
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

# Calculate accuracy for Linear Regression model
lr_accuracy = accuracy_score(y_test <= threshold, lr_predictions <= threshold) * 100

# Train an XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)

# Calculate accuracy for XGBoost model
xgb_accuracy = accuracy_score(y_test <= threshold, xgb_predictions <= threshold) * 100

# Calculate accuracy for the hybrid model
hybrid_accuracy = accuracy_score(y_test <= threshold, hybrid_predictions <= threshold) * 100

# Create a Voting Regressor with Linear Regression and XGBoost models
voting_regressor = VotingRegressor(estimators=[('linear_regression', lr_model), ('xgboost', xgb_model)])

# Fit the ensemble model on the training data
voting_regressor.fit(X_train, y_train)

# Make predictions with the ensemble model
ensemble_predictions = voting_regressor.predict(X_test)

# Calculate accuracy for the ensemble model
ensemble_accuracy = accuracy_score(y_test <= threshold, ensemble_predictions <= threshold) * 100

# Calculate accuracy for the hybrid model without Voting Classifier
hybrid_accuracy_without_voting = accuracy_score(y_test <= threshold, hybrid_predictions <= threshold) * 100

# Print accuracy for Linear Regression model
print("Linear Regression Model Accuracy:", lr_accuracy, "%")

# Print accuracy for XGBoost model
print("XGBoost Model Accuracy:", xgb_accuracy, "%")

# Print accuracy for the ensemble model
print("Ensemble Model (Voting Regressor) Accuracy:", ensemble_accuracy, "%")

# Print accuracy for the hybrid model without Voting Classifier
print("Hybrid Model (Without Voting) Accuracy:", hybrid_accuracy_without_voting, "%")

# Scatter plot of actual vs. predicted values for the hybrid model
plt.figure(figsize=(8, 6))
plt.scatter(y_test, hybrid_predictions, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values (Hybrid)")
plt.title("Scatter Plot")
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset (use your dataset file path)
data = pd.read_csv('/content/encoded2_dataset.csv')

# Specify the columns for the scatter plot
x_column = 'Academic Reputation Score'
y_column = 'Overall SCORE'

# Create a scatter plot using Seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x=x_column, y=y_column)
plt.title(f'Scatter Plot of {x_column} vs. {y_column}')
plt.xlabel(x_column)
plt.ylabel(y_column)
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.ensemble import VotingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix

# Load your dataset
# data = pd.read_csv('/content/countries-aggregated.csv')
data = pd.read_csv('/content/encoded2_dataset.csv')

# Convert 'Date' to datetime and extract numeric features
# data['Date'] = pd.to_datetime(data['Date'])
# data['Year'] = data['Date'].dt.year
# data['Month'] = data['Date'].dt.month
# data['Day'] = data['Date'].dt.day
# data = data.drop(['Date'], axis=1)

# Split the data into features (X) and the target variable (Deaths)
# X = data[['Year', 'Month', 'Day', 'Confirmed', 'Recovered']]
# y = data['Deaths']

X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Initialize models
lr_model = LinearRegression()
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Initialize k-fold cross-validation
k_folds = 5
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store predictions for each fold
lr_predictions = []
xgb_predictions = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train Linear Regression model
    lr_model.fit(X_train, y_train)
    lr_pred = lr_model.predict(X_test)
    lr_predictions.extend(lr_pred)

    # Train XGBoost model
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    xgb_predictions.extend(xgb_pred)

# Calculate RMSE and R-squared for each model
lr_rmse = np.sqrt(mean_squared_error(y, lr_predictions))
lr_r2 = r2_score(y, lr_predictions)

xgb_rmse = np.sqrt(mean_squared_error(y, xgb_predictions))
xgb_r2 = r2_score(y, xgb_predictions)

# Create a Voting Regressor with Linear Regression and XGBoost models
voting_regressor = VotingRegressor(estimators=[('linear_regression', lr_model), ('xgboost', xgb_model)])

# Perform k-fold cross-validation with the ensemble model
ensemble_predictions = cross_val_predict(voting_regressor, X, y, cv=kf)

# Calculate RMSE and R-squared for the ensemble model
ensemble_rmse = np.sqrt(mean_squared_error(y, ensemble_predictions))
ensemble_r2 = r2_score(y, ensemble_predictions)

# Print evaluation metrics for each model and the ensemble
print("Linear Regression Model - RMSE:", lr_rmse, "R-squared:", lr_r2)
print("XGBoost Model - RMSE:", xgb_rmse, "R-squared:", xgb_r2)
print("Ensemble Model - RMSE:", ensemble_rmse, "R-squared:", ensemble_r2)

# Define a threshold for binary classification
threshold = 0.5

# Combine predictions from both models using simple averaging
hybrid_predictions = (np.array(lr_predictions) + np.array(xgb_predictions)) / 2

# Convert hybrid predictions to binary class labels
binary_hybrid_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y to binary labels using the same threshold
binary_y = (y > threshold).astype(int)

# Calculate accuracy for the hybrid model
hybrid_accuracy = accuracy_score(binary_y, binary_hybrid_predictions) * 100

# Print accuracy for the hybrid model
print("Hybrid Model - Accuracy:", hybrid_accuracy, "%")

# Scatter plot of actual vs. predicted values for the ensemble model
plt.scatter(y, ensemble_predictions, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values (Ensemble)")
plt.title("Scatter Plot of Actual vs. Predicted (Ensemble)")
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import xgboost as xgb
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.ensemble import VotingRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report, confusion_matrix

# Load the dataset and preprocess
data = pd.read_csv('/content/encoded2_dataset.csv')

# Split the data into features (X) and target variable (2025 Rank)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_predictions = lr_model.predict(X_test)

# Train an XGBoost model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)
xgb_predictions = xgb_model.predict(X_test)

# Train additional models
svr_model = SVR()
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Create a Voting Regressor with Linear Regression, XGBoost, SVR, and RandomForest models
voting_regressor = VotingRegressor(estimators=[
    ('linear_regression', lr_model),
    ('xgboost', xgb_model),
    ('svr', svr_model),
    ('random_forest', rf_model)
])

# Fit the ensemble model on the training data
voting_regressor.fit(X_train, y_train)

# Make predictions with the ensemble model
ensemble_predictions = voting_regressor.predict(X_test)

# Calculate RMSE for the ensemble model
ensemble_rmse = mean_squared_error(y_test, ensemble_predictions, squared=False)

# Calculate R2 score for the ensemble model
ensemble_r2 = r2_score(y_test, ensemble_predictions)

# Calculate accuracy percentage for the ensemble model
threshold = 0.5  # Set a threshold for ranking prediction
ensemble_accuracy = accuracy_score(y_test <= threshold, ensemble_predictions <= threshold) * 100

# Print evaluation metrics for the ensemble model
print("\nEnsemble Model (Voting Regressor) - RMSE:", ensemble_rmse)
print("Ensemble Model (Voting Regressor) - R2 Score:", ensemble_r2)
print("Ensemble Model (Voting Regressor) - Accuracy:", ensemble_accuracy, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (ensemble_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("\nEnsemble Model (Voting Regressor) - Precision:", precision)
print("Ensemble Model (Voting Regressor) - Recall:", recall)
print("Ensemble Model (Voting Regressor) - F1 Score:", f1)

# Print classification report for the ensemble model
print("\nClassification Report for the Ensemble Model (Voting Regressor):")
print(classification_report(binary_y_test, binary_predictions))

# Print confusion matrix for the hybrid model
print("Confusion Matrix for the Hybrid Model:")
print(confusion_matrix(binary_y_test, binary_predictions))

##DATA AUGEMENTATION
# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load your dataset
data = pd.read_csv('/content/countries-aggregated.csv')

data = data.sample(frac=1, random_state=42)  # Shuffle with a fixed random seed (42)


# Convert 'Date' to datetime and extract numeric features
data['Date'] = pd.to_datetime(data['Date'])
data['Year'] = data['Date'].dt.year
data['Month'] = data['Date'].dt.month
data['Day'] = data['Date'].dt.day

#Drop 'Date' column and use 'Year', 'Month', and 'Day' as features
data = data.drop(['Date'], axis=1)

# Split the data into features (X) and the target variable (Deaths)
X = data[['Year', 'Month', 'Day', 'Confirmed', 'Recovered']]
y = data['Deaths']

# Data cleaning (removing missing values)
X = X.dropna()
y = y[X.index]

# Handling outliers using Isolation Forest
outlier_detector = IsolationForest(contamination=0.05, random_state=42)
outliers = outlier_detector.fit_predict(X)

# Remove outliers from the dataset
X = X[outliers != -1]
y = y[outliers != -1]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (Standardization)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train a Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions
y_pred = lr_model.predict(X_test)

# Calculate RMSE and R-squared
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Print evaluation metrics
print("RMSE:", rmse)
print("R-squared:", r2)

# Scatter plot of actual vs. predicted values
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Scatter Plot of Actual vs. Predicted")
plt.grid(True)
plt.show()

"""**PSO AND TABU SEARCH WITH BASE MODELS**"""

# Install required libraries
!pip install scikit-learn pyswarm tabulate xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from pyswarm import pso
from tabulate import tabulate
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base models
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
xgb_model = xgb.XGBRegressor()
gbm_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
knn_model = KNeighborsRegressor(n_neighbors=5)
svm_model = SVR(kernel='linear')
linear_model = LinearRegression()

base_models = [rf_model, xgb_model, gbm_model, knn_model, svm_model, linear_model]  # Add other base models here if needed

# Train the base models
for model in base_models:
    model.fit(X_train, y_train)

# Define the fitness function for PSO
def fitness_function(weights):
    ensemble_predictions = sum(w * model.predict(X_test) for w, model in zip(weights, base_models))
    mse = mean_squared_error(y_test, ensemble_predictions)
    return mse

# Number of base models
num_models = len(base_models)

# Initialize PSO optimization parameters
lb = [0.0] * num_models  # Lower bounds for weights
ub = [1.0] * num_models  # Upper bounds for weights

# Perform PSO optimization to find optimal weights
best_weights, _ = pso(fitness_function, lb, ub, swarmsize=20, maxiter=50)

# Combine predictions using optimized weights
ensemble_predictions = sum(w * model.predict(X_test) for w, model in zip(best_weights, base_models))

# Calculate RMSE for the ensemble
ensemble_rmse = mean_squared_error(y_test, ensemble_predictions, squared=False)

# Calculate MAE for the ensemble
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)

# Calculate R2 score for the ensemble
ensemble_r2 = r2_score(y_test, ensemble_predictions)

# Calculate accuracy percentage based on RMSE
accuracy_percentage = ((1 - (ensemble_rmse / y_test.mean())) * 100).round(2)

# Display the optimized weights, RMSE, MAE, R2 score, and accuracy percentage
results = [{'Model': f'Model {i+1}', 'Weight': weight} for i, weight in enumerate(best_weights)]
results.append({'Model': 'Ensemble', 'Weight': ''})
results.append({'Model': 'Ensemble RMSE', 'Weight': ensemble_rmse})
results.append({'Model': 'Ensemble MAE', 'Weight': ensemble_mae})
results.append({'Model': 'Ensemble R2 Score', 'Weight': ensemble_r2})
results.append({'Model': 'Accuracy Percentage', 'Weight': accuracy_percentage})

print(tabulate(results, headers='keys', tablefmt='grid'))
print("Accuracy Percentage for Complex Hybrid Model:", accuracy_percentage, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)

"""**PSO and Tabu search with hybrid models(PSO+TS - Hybrid Models)**"""

# Install required libraries
!pip install scikit-learn pyswarm tabulate xgboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from pyswarm import pso
from tabulate import tabulate
import xgboost as xgb
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the dataset (Replace with your dataset)
data = pd.read_csv('/content/encoded2_dataset.csv')

# Splitting data into features (X) and target (y)
X = data[['Academic Reputation Score','Academic Reputation Rank','Employer Reputation Score','Employer Reputation Rank','Faculty Student Score','Faculty Student Rank','Citations per Faculty Score','Citations per Faculty Rank','International Faculty Score','International Faculty Rank','International Students Score','International Students Rank','International Research Network Score','International Research Network Rank','Employment Outcomes Score','Employment Outcomes Rank','Sustainability Score','Sustainability Rank']]
y = data['Overall SCORE']

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define hybrid models
hybrid_models = {
    "XGBoost and Neural Network": [xgb.XGBRegressor(), Sequential([Dense(64, input_dim=X_train.shape[1], activation='relu'), Dense(1, activation='linear')])],
    "Gradient Boosting and k-Nearest Neighbors": [GradientBoostingRegressor(), KNeighborsRegressor(n_neighbors=5)],
    "Random Forest and SVM": [RandomForestRegressor(), SVR(kernel='linear')],
    "SVM, Neural Network, and Gradient Boosting model": [SVR(kernel='linear'), Sequential([Dense(64, input_dim=X_train.shape[1], activation='relu'), Dense(1, activation='linear')]), GradientBoostingRegressor()],
    "RandomForest and XGBoost": [RandomForestRegressor(), xgb.XGBRegressor()],
    "LinearRegression and RandomForest": [LinearRegression(), RandomForestRegressor()],
    "LinearRegression and XGBoost": [LinearRegression(), xgb.XGBRegressor()]
}

# ...

# ...

# ...

# ...

results = []

# Lists to store RMSE, MAE, and R2 scores for each model
rmse_scores = []
mae_scores = []
r2_scores = []

# Iterate through hybrid models
for model_name, model_list in hybrid_models.items():
    base_models = [model for model in model_list]

    # Train the base models and make predictions
    base_predictions = []
    for model in base_models:
        if isinstance(model, Sequential):
            model.compile(optimizer='adam', loss='mean_squared_error')
            predictions = model.predict(X_test)
        else:
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
        base_predictions.append(predictions)

    # Define the fitness function for PSO (optimize for RMSE)
    def fitness_function(weights):
        # Flatten the predictions before summing
        ensemble_predictions = np.sum([w * predictions.flatten() for w, predictions in zip(weights, base_predictions)], axis=0)
        mse = mean_squared_error(y_test, ensemble_predictions)
        return mse

    # Number of base models
    num_models = len(base_models)

    # Initialize PSO optimization parameters
    lb = [0.0] * num_models  # Lower bounds for weights
    ub = [1.0] * num_models  # Upper bounds for weights

    # Perform PSO optimization to find optimal weights
    best_weights, _ = pso(fitness_function, lb, ub, swarmsize=20, maxiter=50)

    # Combine predictions using optimized weights
    ensemble_predictions = np.sum([w * predictions.flatten() for w, predictions in zip(best_weights, base_predictions)], axis=0)

    # Calculate RMSE, MAE, and R2 for the ensemble
    ensemble_rmse = mean_squared_error(y_test, ensemble_predictions, squared=False)
    ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)
    ensemble_r2 = r2_score(y_test, ensemble_predictions)

    # Append RMSE, MAE, and R2 scores to the respective lists
    rmse_scores.append(ensemble_rmse)
    mae_scores.append(ensemble_mae)
    r2_scores.append(ensemble_r2)

    # Calculate accuracy percentage based on RMSE
    accuracy_percentage = ((1 - (ensemble_rmse / y_test.mean())) * 100).round(2)

    # Store results
    result = {
        "Model Pair": model_name,
        "Optimization Method": "PSO",
        "Ensemble RMSE": ensemble_rmse,
        "Ensemble MAE": ensemble_mae,
        "Ensemble R2 Score": ensemble_r2,
        "Accuracy Percentage": accuracy_percentage
    }
    results.append(result)

# Calculate and print overall combined RMSE, MAE, and R2 scores
overall_rmse = np.mean(rmse_scores)
overall_mae = np.mean(mae_scores)
overall_r2 = np.mean(r2_scores)
print("Overall Combined RMSE:", overall_rmse)
print("Overall Combined MAE:", overall_mae)
print("Overall Combined R2 Score:", overall_r2)

# Print the results for each hybrid model
result_df = pd.DataFrame(results)
print("Results for Hybrid Models with Particle Swarm Optimization:")
print(result_df)

# Calculate accuracy for the final hybrid model
final_hybrid_accuracy = result_df[result_df["Model Pair"] == "SVM, Neural Network, and Gradient Boosting model"]["Accuracy Percentage"].values[0]
print("Accuracy Percentage for the Final Hybrid Model:", final_hybrid_accuracy, "%")

# Threshold for binary classification
threshold = 0.5

# Convert predictions to binary class labels based on the threshold
binary_predictions = (hybrid_predictions > threshold).astype(int)

# Convert y_test to binary labels using the same threshold
binary_y_test = (y_test > threshold).astype(int)

# Calculate Precision
precision = precision_score(binary_y_test, binary_predictions)

# Calculate Recall
recall = recall_score(binary_y_test, binary_predictions)

# Calculate F1 Score
f1 = f1_score(binary_y_test, binary_predictions)

# Print the evaluation metrics including precision, recall, and F1 Score
print("Hybrid Model - RMSE:", hybrid_rmse, " - MAE:", hybrid_mae, " - R2 Score:", hybrid_r2, " - Accuracy Percentage:", hybrid_accuracy_percentage, "%")
print("Hybrid Model - Precision:", precision, " - Recall:", recall, " - F1 Score:", f1)